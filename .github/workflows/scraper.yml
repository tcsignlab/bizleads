name: BizLeads Scraper

on:
  schedule:
    - cron: "0 2 * * *"

  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode"
        required: true
        default: "daily"
        type: choice
        options:
          - daily
          - reset
          - dry-run

permissions:
  contents: write

jobs:
  scrape:
    name: Run BizLeads Scrapers
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium --with-deps

      # Always remove the stale aggregate — it gets rebuilt fresh after every scrape
      - name: Remove stale businesses.json
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        run: rm -f data/businesses.json

      # RESET only: wipe all individual state data too
      - name: Wipe all state data (reset only)
        if: ${{ github.event.inputs.mode == 'reset' }}
        run: |
          rm -f data/*_scraper_state.json
          rm -f data/*_businesses.json
          echo "All state data wiped"

      - name: Dry run preview
        if: ${{ github.event.inputs.mode == 'dry-run' }}
        run: python3 reset_and_refresh.py --dry-run

      # RESET: 3000 per state, no existing data to load
      - name: Run scrapers - RESET (3000 per state)
        if: ${{ github.event.inputs.mode == 'reset' }}
        run: |
          python3 - <<'EOF'
          import subprocess, sys, os, glob, json

          schedulers = sorted(glob.glob('*_scheduler.py'))
          schedulers = [s for s in schedulers if s != 'multi_state_scheduler.py']
          print(f'RESET: {len(schedulers)} states x 3000 records each')
          os.makedirs('data', exist_ok=True)
          failed = []

          for script in schedulers:
              label = script.replace('_scheduler.py','').replace('_',' ').title()
              result = subprocess.run(
                  [sys.executable, script, '--once', '--count', '3000'],
                  capture_output=True, text=True, timeout=180
              )
              if result.returncode == 0:
                  print(f'  v {label}')
              else:
                  print(f'  x {label} (exit {result.returncode})')
                  if result.stderr:
                      print(f'    {result.stderr.strip()[:300]}')
                  failed.append(label)

          # Dedup + rebuild aggregate
          all_biz = []
          for filepath in sorted(glob.glob('data/*_businesses.json')):
              bizs = json.load(open(filepath))
              seen, unique = set(), []
              for b in sorted(bizs, key=lambda x: x.get('scraped_at',''), reverse=True):
                  k = str(b.get('state','')) + '_' + str(b.get('entity_number','')) if b.get('entity_number') else b.get('business_id') or b.get('name','')
                  if k and k not in seen:
                      seen.add(k); unique.append(b)
              unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)
              json.dump(unique, open(filepath,'w'), indent=2)
              print(f'  {os.path.basename(filepath)}: {len(unique)} records')
              all_biz.extend(unique)

          seen, unique = set(), []
          for b in sorted(all_biz, key=lambda x: x.get('scraped_at',''), reverse=True):
              k = str(b.get('state','')) + '_' + str(b.get('entity_number','')) if b.get('entity_number') else b.get('business_id') or b.get('name','')
              if k and k not in seen:
                  seen.add(k); unique.append(b)
          unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)
          json.dump(unique, open('data/businesses.json','w'), indent=2)
          mb = os.path.getsize('data/businesses.json') / 1024 / 1024
          print(f'\nbusinesses.json: {len(unique)} records ({mb:.1f} MB)')
          if failed:
              print(f'WARNING: failed: {", ".join(failed)}'); sys.exit(1)
          EOF

      # DAILY: keep existing state files, add 3000 new per state
      - name: Run scrapers - DAILY (add 3000 new per state)
        if: ${{ github.event_name == 'schedule' || github.event.inputs.mode == 'daily' }}
        run: |
          python3 - <<'EOF'
          import subprocess, sys, os, glob, json

          schedulers = sorted(glob.glob('*_scheduler.py'))
          schedulers = [s for s in schedulers if s != 'multi_state_scheduler.py']
          print(f'DAILY: {len(schedulers)} states, adding 3000 new each')
          os.makedirs('data', exist_ok=True)
          failed = []

          for script in schedulers:
              label = script.replace('_scheduler.py','').replace('_',' ').title()
              # Remove timer state file only — keep existing business data
              state_key = script.replace('_scheduler.py', '')
              state_file = os.path.join('data', f'{state_key}_scraper_state.json')
              if os.path.exists(state_file):
                  os.remove(state_file)

              result = subprocess.run(
                  [sys.executable, script, '--once', '--count', '3000'],
                  capture_output=True, text=True, timeout=180
              )
              if result.returncode == 0:
                  print(f'  v {label}')
              else:
                  print(f'  x {label} (exit {result.returncode})')
                  if result.stderr:
                      print(f'    {result.stderr.strip()[:300]}')
                  failed.append(label)

          # Dedup each state file then rebuild aggregate
          all_biz = []
          total_removed = 0
          for filepath in sorted(glob.glob('data/*_businesses.json')):
              bizs = json.load(open(filepath))
              before = len(bizs)
              seen, unique = set(), []
              for b in sorted(bizs, key=lambda x: x.get('scraped_at',''), reverse=True):
                  k = str(b.get('state','')) + '_' + str(b.get('entity_number','')) if b.get('entity_number') else b.get('business_id') or b.get('name','')
                  if k and k not in seen:
                      seen.add(k); unique.append(b)
              unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)
              removed = before - len(unique)
              total_removed += removed
              json.dump(unique, open(filepath,'w'), indent=2)
              dupes = f' (-{removed} dupes)' if removed else ''
              print(f'  {os.path.basename(filepath)}: {len(unique)} records{dupes}')
              all_biz.extend(unique)

          if total_removed:
              print(f'  Total dupes removed: {total_removed}')

          seen, unique = set(), []
          for b in sorted(all_biz, key=lambda x: x.get('scraped_at',''), reverse=True):
              k = str(b.get('state','')) + '_' + str(b.get('entity_number','')) if b.get('entity_number') else b.get('business_id') or b.get('name','')
              if k and k not in seen:
                  seen.add(k); unique.append(b)
          unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)
          json.dump(unique, open('data/businesses.json','w'), indent=2)
          mb = os.path.getsize('data/businesses.json') / 1024 / 1024
          print(f'\nbusinesses.json: {len(unique)} records ({mb:.1f} MB)')
          if failed:
              print(f'WARNING: failed: {", ".join(failed)}'); sys.exit(1)
          EOF

      - name: Upload data artifact
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        uses: actions/upload-artifact@v4
        with:
          name: bizleads-data-${{ github.run_number }}
          path: data/
          retention-days: 7

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: "*.log"
          retention-days: 7

      - name: Commit updated data to repo
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/
          git diff --staged --quiet || git commit -m "chore: update scraper data [skip ci]"
          git push
