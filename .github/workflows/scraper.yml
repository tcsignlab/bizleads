name: BizLeads Scraper

on:
  # Run daily at 2:00 AM UTC
  schedule:
    - cron: "0 2 * * *"

  # Allow manual trigger from GitHub Actions UI
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode"
        required: true
        default: "daily"
        type: choice
        options:
          - daily       # normal daily run (adds new, deduplicates)
          - reset       # wipe everything and start fresh
          - dry-run     # preview only, no changes

permissions:
  contents: write

jobs:
  scrape:
    name: Run BizLeads Scrapers
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 hour cap for all 50 states

    steps:
      # ── 1. Checkout repo ──────────────────────────────────────────────
      - name: Checkout repository
        uses: actions/checkout@v4

      # ── 2. Set up Python ──────────────────────────────────────────────
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      # ── 3. Install dependencies ───────────────────────────────────────
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium --with-deps

      # ── 4. Restore cached data between runs ───────────────────────────
      #       Skip restore on reset so we start clean
      - name: Restore data cache
        if: ${{ github.event.inputs.mode != 'reset' }}
        uses: actions/cache@v4
        with:
          path: data/
          key: bizleads-data-${{ runner.os }}
          restore-keys: |
            bizleads-data-

      # ── 5a. Full reset + fresh scrape ─────────────────────────────────
      - name: Reset and scrape fresh (reset mode)
        if: ${{ github.event.inputs.mode == 'reset' }}
        run: python3 reset_and_refresh.py

      # ── 5b. Dry run (preview only) ────────────────────────────────────
      - name: Dry run preview
        if: ${{ github.event.inputs.mode == 'dry-run' }}
        run: python3 reset_and_refresh.py --dry-run

      # ── 5c. Normal daily run (scheduled or manual daily) ──────────────
      - name: Daily scrape (add new, deduplicate)
        if: ${{ github.event_name == 'schedule' || github.event.inputs.mode == 'daily' }}
        run: |
          python3 -c "
          import subprocess, sys, os, glob, json
          from datetime import datetime

          schedulers = sorted(glob.glob('*_scheduler.py'))
          # Exclude multi_state — we run each state directly
          schedulers = [s for s in schedulers if s != 'multi_state_scheduler.py']

          print(f'Running {len(schedulers)} state scrapers...')
          failed = []
          for script in schedulers:
              state = script.replace('_scheduler.py','').replace('_',' ').title()
              result = subprocess.run(
                  [sys.executable, script, '--once'],
                  capture_output=True, text=True, timeout=120
              )
              if result.returncode == 0:
                  print(f'  ✓ {state}')
              else:
                  print(f'  ✗ {state} (exit {result.returncode})')
                  failed.append(state)

          # Deduplicate all data files
          for filepath in glob.glob('data/*_businesses.json'):
              try:
                  with open(filepath) as f:
                      bizs = json.load(f)
                  seen, unique = set(), []
                  bizs.sort(key=lambda x: x.get('scraped_at',''), reverse=True)
                  for b in bizs:
                      k = b.get('entity_number') or b.get('business_id') or b.get('name','')
                      if k and k not in seen:
                          seen.add(k)
                          unique.append(b)
                  unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)
                  removed = len(bizs) - len(unique)
                  with open(filepath, 'w') as f:
                      json.dump(unique, f, indent=2)
                  if removed:
                      print(f'  Deduped {os.path.basename(filepath)}: removed {removed}')
              except Exception as e:
                  print(f'  Dedup error {filepath}: {e}')

          if failed:
              print(f'WARNING: {len(failed)} states failed: {failed}')
              sys.exit(1)
          print('All done.')
          "

      # ── 6. Save data cache for next run ───────────────────────────────
      - name: Save data cache
        if: ${{ github.event.inputs.mode != 'dry-run' && always() }}
        uses: actions/cache@v4
        with:
          path: data/
          key: bizleads-data-${{ runner.os }}

      # ── 7. Upload data as artifact (keeps last 7 days) ────────────────
      - name: Upload data artifact
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        uses: actions/upload-artifact@v4
        with:
          name: bizleads-data-${{ github.run_number }}
          path: data/
          retention-days: 7

      # ── 8. Upload logs ────────────────────────────────────────────────
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: "*.log"
          retention-days: 7

      # ── 9. Commit updated data back to repo (optional) ────────────────
      #       Remove this step if you don't want data committed to git
      - name: Commit data to repo
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/
          git diff --staged --quiet || git commit -m "chore: update scraper data [skip ci]"
          git push
