name: BizLeads Scraper

on:
  schedule:
    - cron: "0 2 * * *"

  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode"
        required: true
        default: "daily"
        type: choice
        options:
          - daily
          - reset
          - dry-run

permissions:
  contents: write

jobs:
  scrape:
    name: Run BizLeads Scrapers
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium --with-deps

      # RESET: wipe everything clean before scraping fresh
      - name: Wipe all data (reset mode only)
        if: ${{ github.event.inputs.mode == 'reset' }}
        run: |
          rm -f data/*_scraper_state.json
          rm -f data/*_businesses.json
          rm -f data/businesses.json
          echo "All state data wiped for fresh start"

      - name: Dry run preview
        if: ${{ github.event.inputs.mode == 'dry-run' }}
        run: python3 reset_and_refresh.py --dry-run

      # RESET: scrape 3000 per state fresh
      - name: Run scrapers - RESET (3000 per state)
        if: ${{ github.event.inputs.mode == 'reset' }}
        run: |
          python3 - <<'EOF'
          import subprocess, sys, os, glob, json

          schedulers = sorted(glob.glob('*_scheduler.py'))
          schedulers = [s for s in schedulers if s != 'multi_state_scheduler.py']
          print(f'RESET: Running {len(schedulers)} state scrapers at 3000 per state...')
          os.makedirs('data', exist_ok=True)
          failed = []

          for script in schedulers:
              state = script.replace('_scheduler.py','').replace('_',' ').title()
              result = subprocess.run(
                  [sys.executable, script, '--once', '--count', '3000'],
                  capture_output=True, text=True, timeout=180
              )
              if result.returncode == 0:
                  print(f'  v {state}')
              else:
                  print(f'  x {state} (exit {result.returncode})')
                  if result.stderr:
                      print(f'    {result.stderr.strip()[:300]}')
                  failed.append(state)

          # Dedup each state file
          for filepath in sorted(glob.glob('data/*_businesses.json')):
              try:
                  bizs = json.load(open(filepath))
                  seen, unique = set(), []
                  for b in sorted(bizs, key=lambda x: x.get('scraped_at',''), reverse=True):
                      k = b.get('entity_number') or b.get('business_id') or b.get('name','')
                      if k and k not in seen:
                          seen.add(k)
                          unique.append(b)
                  unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)
                  json.dump(unique, open(filepath,'w'), indent=2)
                  print(f'  {os.path.basename(filepath)}: {len(unique)} records')
              except Exception as e:
                  print(f'  ERROR {filepath}: {e}')

          # Rebuild aggregate
          all_biz = []
          for filepath in sorted(glob.glob('data/*_businesses.json')):
              all_biz.extend(json.load(open(filepath)))
          seen, unique = set(), []
          for b in sorted(all_biz, key=lambda x: x.get('scraped_at',''), reverse=True):
              k = b.get('entity_number') or b.get('business_id') or b.get('name','')
              if k and k not in seen:
                  seen.add(k)
                  unique.append(b)
          unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)
          json.dump(unique, open('data/businesses.json','w'), indent=2)
          mb = os.path.getsize('data/businesses.json') / 1024 / 1024
          print(f'\nbusinesses.json: {len(unique)} total records ({mb:.1f} MB)')
          if failed:
              print(f'WARNING: failed states: {", ".join(failed)}')
              sys.exit(1)
          EOF

      # DAILY: add 3000 new results per state to existing data, no wipe
      - name: Run scrapers - DAILY (add 3000 new per state)
        if: ${{ github.event_name == 'schedule' || github.event.inputs.mode == 'daily' }}
        run: |
          python3 - <<'EOF'
          import subprocess, sys, os, glob, json

          schedulers = sorted(glob.glob('*_scheduler.py'))
          schedulers = [s for s in schedulers if s != 'multi_state_scheduler.py']
          print(f'DAILY: Running {len(schedulers)} state scrapers, adding 3000 new per state...')
          os.makedirs('data', exist_ok=True)
          failed = []

          for script in schedulers:
              state = script.replace('_scheduler.py','').replace('_',' ').title()

              # Remove only the state file (timer guard) â€” keep existing business data
              state_key = script.replace('_scheduler.py', '')
              state_file = os.path.join('data', f'{state_key}_scraper_state.json')
              if os.path.exists(state_file):
                  os.remove(state_file)

              result = subprocess.run(
                  [sys.executable, script, '--once', '--count', '3000'],
                  capture_output=True, text=True, timeout=180
              )
              if result.returncode == 0:
                  print(f'  v {state}')
              else:
                  print(f'  x {state} (exit {result.returncode})')
                  if result.stderr:
                      print(f'    {result.stderr.strip()[:300]}')
                  failed.append(state)

          # Dedup each state file (removes any dupes from the merge)
          total_removed = 0
          for filepath in sorted(glob.glob('data/*_businesses.json')):
              try:
                  bizs = json.load(open(filepath))
                  before = len(bizs)
                  seen, unique = set(), []
                  for b in sorted(bizs, key=lambda x: x.get('scraped_at',''), reverse=True):
                      k = b.get('entity_number') or b.get('business_id') or b.get('name','')
                      if k and k not in seen:
                          seen.add(k)
                          unique.append(b)
                  unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)
                  removed = before - len(unique)
                  total_removed += removed
                  json.dump(unique, open(filepath,'w'), indent=2)
                  dupes = f' (-{removed} dupes)' if removed else ''
                  print(f'  {os.path.basename(filepath)}: {len(unique)} records{dupes}')
              except Exception as e:
                  print(f'  ERROR {filepath}: {e}')

          if total_removed:
              print(f'  Total duplicates removed: {total_removed}')

          # Rebuild aggregate businesses.json
          all_biz = []
          for filepath in sorted(glob.glob('data/*_businesses.json')):
              all_biz.extend(json.load(open(filepath)))
          seen, unique = set(), []
          for b in sorted(all_biz, key=lambda x: x.get('scraped_at',''), reverse=True):
              k = b.get('entity_number') or b.get('business_id') or b.get('name','')
              if k and k not in seen:
                  seen.add(k)
                  unique.append(b)
          unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)
          json.dump(unique, open('data/businesses.json','w'), indent=2)
          mb = os.path.getsize('data/businesses.json') / 1024 / 1024
          print(f'\nbusinesses.json: {len(unique)} total records ({mb:.1f} MB)')
          if failed:
              print(f'WARNING: failed states: {", ".join(failed)}')
              sys.exit(1)
          EOF

      - name: Upload data artifact
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        uses: actions/upload-artifact@v4
        with:
          name: bizleads-data-${{ github.run_number }}
          path: data/
          retention-days: 7

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: "*.log"
          retention-days: 7

      - name: Commit updated data to repo
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/
          git diff --staged --quiet || git commit -m "chore: update scraper data [skip ci]"
          git push
