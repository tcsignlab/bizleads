name: BizLeads Scraper

on:
  schedule:
    - cron: "0 2 * * *"

  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode"
        required: true
        default: "daily"
        type: choice
        options:
          - daily
          - reset
          - dry-run

permissions:
  contents: write

jobs:
  scrape:
    name: Run BizLeads Scrapers
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium --with-deps

      # Always delete the stale businesses.json â€” rebuilt fresh after every scrape
      - name: Clear stale aggregate file
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        run: |
          rm -f data/businesses.json
          echo "Cleared stale businesses.json"

      # On reset: also wipe all state scraper files and individual state data
      - name: Wipe all data (reset mode only)
        if: ${{ github.event.inputs.mode == 'reset' }}
        run: |
          rm -f data/*_scraper_state.json
          rm -f data/*_businesses.json
          echo "All state data wiped for fresh start"

      - name: Dry run preview
        if: ${{ github.event.inputs.mode == 'dry-run' }}
        run: python3 reset_and_refresh.py --dry-run

      - name: Run all 50 state scrapers
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        run: |
          python3 - <<'EOF'
          import subprocess, sys, os, glob, json

          schedulers = sorted(glob.glob('*_scheduler.py'))
          schedulers = [s for s in schedulers if s != 'multi_state_scheduler.py']
          print(f'Running {len(schedulers)} state scrapers...')

          os.makedirs('data', exist_ok=True)
          failed = []

          for script in schedulers:
              state = script.replace('_scheduler.py','').replace('_',' ').title()

              # Delete state file so the 24h guard never blocks execution
              state_key = script.replace('_scheduler.py', '')
              state_file = os.path.join('data', f'{state_key}_scraper_state.json')
              if os.path.exists(state_file):
                  os.remove(state_file)

              result = subprocess.run(
                  [sys.executable, script, '--once'],
                  capture_output=True, text=True, timeout=120
              )
              if result.returncode == 0:
                  print(f'  v {state}')
              else:
                  print(f'  x {state} (exit {result.returncode})')
                  if result.stderr:
                      print(f'    {result.stderr.strip()[:300]}')
                  failed.append(state)

          if failed:
              print(f'\nWARNING: {len(failed)} states failed: {", ".join(failed)}')

          # Deduplicate each individual state file
          print('\nDeduplicating state files...')
          for filepath in sorted(glob.glob('data/*_businesses.json')):
              try:
                  bizs = json.load(open(filepath))
                  seen, unique = set(), []
                  bizs.sort(key=lambda x: x.get('scraped_at',''), reverse=True)
                  for b in bizs:
                      k = b.get('entity_number') or b.get('business_id') or b.get('name','')
                      if k and k not in seen:
                          seen.add(k)
                          unique.append(b)
                  unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)
                  removed = len(bizs) - len(unique)
                  json.dump(unique, open(filepath,'w'), indent=2)
                  name = os.path.basename(filepath)
                  dupes = f' ({removed} dupes removed)' if removed else ''
                  print(f'  {name}: {len(unique)} records{dupes}')
              except Exception as e:
                  print(f'  ERROR {filepath}: {e}')

          # Rebuild businesses.json from all state files
          print('\nRebuilding businesses.json...')
          all_biz = []
          for filepath in sorted(glob.glob('data/*_businesses.json')):
              try:
                  bizs = json.load(open(filepath))
                  all_biz.extend(bizs)
              except Exception as e:
                  print(f'  Error reading {filepath}: {e}')

          seen, unique = set(), []
          all_biz.sort(key=lambda x: x.get('scraped_at',''), reverse=True)
          for b in all_biz:
              k = b.get('entity_number') or b.get('business_id') or b.get('name','')
              if k and k not in seen:
                  seen.add(k)
                  unique.append(b)
          unique.sort(key=lambda x: x.get('registration_date',''), reverse=True)

          json.dump(unique, open('data/businesses.json','w'), indent=2)
          size_mb = os.path.getsize('data/businesses.json') / 1024 / 1024
          print(f'businesses.json: {len(unique)} total records ({size_mb:.1f} MB)')

          if failed:
              sys.exit(1)
          print('\nAll done.')
          EOF

      - name: Upload data artifact
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        uses: actions/upload-artifact@v4
        with:
          name: bizleads-data-${{ github.run_number }}
          path: data/
          retention-days: 7

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_number }}
          path: "*.log"
          retention-days: 7

      - name: Commit updated data to repo
        if: ${{ github.event.inputs.mode != 'dry-run' }}
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/
          git diff --staged --quiet || git commit -m "chore: update scraper data [skip ci]"
          git push
